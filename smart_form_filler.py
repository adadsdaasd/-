"""
æ™ºèƒ½å¡«è¡¨æ¨¡å— (Smart Form Filler)
================================
åŠŸèƒ½ï¼š
1. ä¸Šä¼ ç©ºè¡¨æ ¼/æ–‡æ¡£ï¼ŒAI è‡ªåŠ¨è¯†åˆ«éœ€è¦å¡«å†™çš„å­—æ®µ
2. æ ¹æ®ç”¨æˆ·æ¡£æ¡ˆæ™ºèƒ½åŒ¹é…æ•°æ®
3. AI æ¶¦è‰²è¯æœ¯ï¼ˆè®©å›ç­”æ›´ä¸“ä¸šï¼‰
4. æ”¯æŒ Excelã€Wordã€çº¯æ–‡æœ¬é—®é¢˜
"""

import streamlit as st
import pandas as pd
import json
import re
import io
from typing import Dict, List, Tuple, Optional
from datetime import datetime

from openpyxl import load_workbook
from openpyxl.styles import Font, Alignment

try:
    from docx import Document
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False

from ai_services import create_ai_client, DEFAULT_MODEL

from research_models import (
    load_research_profiles,
    get_research_profile_by_id,
    flatten_profile_for_template,
    get_publications_summary,
    get_grants_summary
)
from profile_validation import validate_general_profile, validate_research_profile

# ==================== å¸¸é‡ ====================
# è¯´æ˜ï¼šLLM base_url / model / client ç»Ÿä¸€ç”± ai_services.py ç®¡ç†


def ai_identify_fields(client, content: str) -> List[Dict]:
    """
    AI è¯†åˆ«è¡¨æ ¼/æ–‡æ¡£ä¸­éœ€è¦å¡«å†™çš„å­—æ®µ
    
    è¿”å›: [{"field": "å­—æ®µå", "type": "factual/subjective", "description": "è¯´æ˜"}]
    """
    prompt = """ä½ æ˜¯ä¸€ä¸ªè¡¨å•åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹è¡¨æ ¼/æ–‡æ¡£å†…å®¹ï¼Œè¯†åˆ«å‡ºæ‰€æœ‰éœ€è¦å¡«å†™çš„å­—æ®µã€‚

å¯¹äºæ¯ä¸ªå­—æ®µï¼Œåˆ¤æ–­å®ƒæ˜¯ï¼š
1. factualï¼ˆäº‹å®ç±»ï¼‰ï¼šå¦‚å§“åã€ç”µè¯ã€å­¦å†ç­‰ï¼Œéœ€è¦å‡†ç¡®çš„äº‹å®ä¿¡æ¯
2. subjectiveï¼ˆä¸»è§‚ç±»ï¼‰ï¼šå¦‚è‡ªæˆ‘ä»‹ç»ã€ä¸ªäººä¼˜åŠ¿ã€ç ”ç©¶è®¡åˆ’ç­‰ï¼Œéœ€è¦æ¶¦è‰²çš„æ–‡å­—æè¿°

è¿”å› JSON æ•°ç»„æ ¼å¼ï¼Œä¸è¦æ·»åŠ å…¶ä»–æ–‡å­—ï¼š
[
  {"field": "å­—æ®µå", "type": "factual", "description": "ç®€çŸ­è¯´æ˜"},
  {"field": "å­—æ®µå", "type": "subjective", "description": "ç®€çŸ­è¯´æ˜"}
]

æ–‡æ¡£å†…å®¹ï¼š
"""
    
    try:
        response = client.chat.completions.create(
            model=DEFAULT_MODEL,
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": content[:3000]}  # é™åˆ¶é•¿åº¦
            ],
            temperature=0.3,
            max_tokens=2000
        )
        
        result = response.choices[0].message.content.strip()
        
        # æ¸…ç† markdown ä»£ç å—
        if result.startswith("```"):
            result = re.sub(r'^```(?:json)?\n?', '', result)
            result = re.sub(r'\n?```$', '', result)
        
        return json.loads(result)
    
    except Exception as e:
        st.error(f"AI è¯†åˆ«å­—æ®µå¤±è´¥: {str(e)}")
        return []


def ai_generate_answer(client, field: str, field_type: str, profile_data: Dict, context: str = "") -> str:
    """
    AI æ ¹æ®å­—æ®µå’Œç”¨æˆ·æ•°æ®ç”Ÿæˆç­”æ¡ˆï¼ˆå•ä¸ªå­—æ®µï¼‰
    """
    profile_summary = _build_profile_summary(profile_data)
    
    if field_type == "factual":
        prompt = f"""æ ¹æ®ä»¥ä¸‹ç”¨æˆ·ä¿¡æ¯ï¼Œä¸º"{field}"å­—æ®µæä¾›å‡†ç¡®çš„ç­”æ¡ˆã€‚
å¦‚æœä¿¡æ¯ä¸å­˜åœ¨ï¼Œå›å¤"æœªæä¾›"ã€‚åªè¿”å›ç­”æ¡ˆï¼Œä¸è¦è§£é‡Šã€‚

ç”¨æˆ·ä¿¡æ¯ï¼š
{profile_summary}

å­—æ®µï¼š{field}
ç­”æ¡ˆï¼š"""
    else:
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ä¹¦æ¶¦è‰²ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·ä¿¡æ¯ï¼Œä¸º"{field}"å­—æ®µæ’°å†™ä¸€æ®µä¸“ä¸šã€å¾—ä½“çš„å›ç­”ã€‚

è¦æ±‚ï¼š
1. è¯­è¨€æ­£å¼ä½†ä¸ç”Ÿç¡¬
2. çªå‡ºä¼˜åŠ¿å’Œäº®ç‚¹
3. ç¬¦åˆå­¦æœ¯/èŒåœºè§„èŒƒ
4. 100-200å­—å·¦å³ï¼ˆé™¤éæ˜¯ç®€çŸ­å­—æ®µï¼‰

ç”¨æˆ·ä¿¡æ¯ï¼š
{profile_summary}

{f"è¡¥å……ä¸Šä¸‹æ–‡ï¼š{context}" if context else ""}

å­—æ®µï¼š{field}
ä¸“ä¸šå›ç­”ï¼š"""
    
    try:
        response = client.chat.completions.create(
            model=DEFAULT_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5 if field_type == "subjective" else 0.2,
            max_tokens=500
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[ç”Ÿæˆå¤±è´¥: {str(e)}]"


def ai_batch_generate_answers(client, fields: List[Dict], profile_data: Dict, style: str = "professional") -> Dict[str, str]:
    """
    æ‰¹é‡ç”Ÿæˆæ‰€æœ‰å­—æ®µçš„ç­”æ¡ˆï¼ˆä¸€æ¬¡ API è°ƒç”¨ï¼Œå¤§å¹…æå‡é€Ÿåº¦ï¼‰
    
    Args:
        client: AI å®¢æˆ·ç«¯
        fields: å­—æ®µåˆ—è¡¨ [{"field": "å­—æ®µå", "type": "factual/subjective"}]
        profile_data: ç”¨æˆ·æ¡£æ¡ˆæ•°æ®
        style: æ¶¦è‰²é£æ ¼
    
    Returns:
        {"å­—æ®µå": "ç­”æ¡ˆ", ...}
    """
    profile_summary = _build_profile_summary(profile_data)
    
    # æ„å»ºå­—æ®µåˆ—è¡¨æè¿°
    fields_desc = []
    for i, f in enumerate(fields, 1):
        field_name = f["field"]
        field_type = f.get("type", "subjective")
        type_hint = "ï¼ˆäº‹å®ç±»ï¼Œç›´æ¥ä»ä¿¡æ¯ä¸­æå–ï¼‰" if field_type == "factual" else "ï¼ˆä¸»è§‚ç±»ï¼Œéœ€è¦æ¶¦è‰²æ’°å†™ï¼‰"
        fields_desc.append(f"{i}. {field_name} {type_hint}")
    
    fields_text = "\n".join(fields_desc)
    
    style_hints = {
        "professional": "ä¸“ä¸šæ­£å¼ï¼Œé€‚åˆèŒåœºå’Œå•†åŠ¡åœºåˆ",
        "academic": "å­¦æœ¯è§„èŒƒï¼Œé€‚åˆè®ºæ–‡å’Œå­¦æœ¯ç”³è¯·",
        "friendly": "äº²å’Œå‹å¥½ï¼Œé€‚åˆè‡ªæˆ‘ä»‹ç»å’Œé¢è¯•"
    }
    style_hint = style_hints.get(style, "ä¸“ä¸šæ­£å¼")
    
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¡¨å•å¡«å†™åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ç”¨æˆ·ä¿¡æ¯ï¼Œä¸ºæ‰€æœ‰å­—æ®µç”Ÿæˆç­”æ¡ˆã€‚

ã€ç”¨æˆ·ä¿¡æ¯ã€‘
{profile_summary}

ã€éœ€è¦å¡«å†™çš„å­—æ®µã€‘
{fields_text}

ã€è¦æ±‚ã€‘
1. äº‹å®ç±»å­—æ®µï¼šç›´æ¥ä»ç”¨æˆ·ä¿¡æ¯ä¸­æå–å‡†ç¡®ç­”æ¡ˆï¼Œæ²¡æœ‰åˆ™å¡«"æœªæä¾›"
2. ä¸»è§‚ç±»å­—æ®µï¼šæ ¹æ®ç”¨æˆ·ä¿¡æ¯æ’°å†™ä¸“ä¸šã€å¾—ä½“çš„å›ç­”ï¼ˆ{style_hint}ï¼‰ï¼Œ100-200å­—
3. çªå‡ºç”¨æˆ·çš„ä¼˜åŠ¿å’Œäº®ç‚¹
4. è¿”å›æ ¼å¼ï¼šä¸¥æ ¼æŒ‰ JSON æ ¼å¼è¿”å›ï¼Œé”®ä¸ºå­—æ®µåï¼Œå€¼ä¸ºç­”æ¡ˆ

ã€è¿”å›æ ¼å¼ç¤ºä¾‹ã€‘
{{
  "å§“å": "å¼ ä¸‰",
  "ä¸ªäººä¼˜åŠ¿": "å…·æœ‰ä¸°å¯Œçš„..."
}}

è¯·ç›´æ¥è¿”å› JSONï¼Œä¸è¦æ·»åŠ å…¶ä»–æ–‡å­—ï¼š"""

    try:
        response = client.chat.completions.create(
            model=DEFAULT_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4,
            max_tokens=3000
        )
        
        result = response.choices[0].message.content.strip()
        
        # æ¸…ç† markdown ä»£ç å—
        if result.startswith("```"):
            result = re.sub(r'^```(?:json)?\n?', '', result)
            result = re.sub(r'\n?```$', '', result)
        
        answers = json.loads(result)
        return answers
    
    except json.JSONDecodeError:
        # JSON è§£æå¤±è´¥ï¼Œè¿”å›ç©ºå­—å…¸ï¼Œè®©è°ƒç”¨æ–¹å›é€€åˆ°é€ä¸ªç”Ÿæˆ
        return {}
    except Exception as e:
        return {"_error": f"æ‰¹é‡ç”Ÿæˆå¤±è´¥: {str(e)}"}


def ai_polish_text(client, original_text: str, style: str = "professional") -> str:
    """
    AI æ¶¦è‰²æ–‡æœ¬
    
    Args:
        original_text: åŸå§‹æ–‡æœ¬
        style: é£æ ¼ - professional/academic/friendly
    """
    style_prompts = {
        "professional": "ä¸“ä¸šæ­£å¼ï¼Œé€‚åˆèŒåœºå’Œå•†åŠ¡åœºåˆ",
        "academic": "å­¦æœ¯è§„èŒƒï¼Œé€‚åˆè®ºæ–‡å’Œå­¦æœ¯ç”³è¯·",
        "friendly": "äº²å’Œå‹å¥½ï¼Œé€‚åˆè‡ªæˆ‘ä»‹ç»å’Œé¢è¯•"
    }
    
    prompt = f"""è¯·æ¶¦è‰²ä»¥ä¸‹æ–‡æœ¬ï¼Œä½¿å…¶{style_prompts.get(style, "æ›´åŠ ä¸“ä¸šå¾—ä½“")}ã€‚

è¦æ±‚ï¼š
1. ä¿æŒåŸæ„ä¸å˜
2. æ”¹å–„è¡¨è¾¾æ–¹å¼
3. å¢å¼ºè¯´æœåŠ›
4. ä¿®æ­£è¯­æ³•é”™è¯¯

åŸæ–‡ï¼š
{original_text}

æ¶¦è‰²åï¼š"""
    
    try:
        response = client.chat.completions.create(
            model=DEFAULT_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.6,
            max_tokens=1000
        )
        
        return response.choices[0].message.content.strip()
    
    except Exception as e:
        return original_text


def _build_profile_summary(profile_data: Dict) -> str:
    """æ„å»ºç”¨æˆ·ä¿¡æ¯æ‘˜è¦"""
    lines = []
    
    # åŸºæœ¬ä¿¡æ¯
    if profile_data.get("å§“å"):
        lines.append(f"å§“åï¼š{profile_data['å§“å']}")
    
    contact = profile_data.get("è”ç³»æ–¹å¼", {})
    if isinstance(contact, dict):
        if contact.get("ç”µè¯"):
            lines.append(f"ç”µè¯ï¼š{contact['ç”µè¯']}")
        if contact.get("é‚®ç®±"):
            lines.append(f"é‚®ç®±ï¼š{contact['é‚®ç®±']}")
    
    # æ•™è‚²èƒŒæ™¯
    education = profile_data.get("education_history", [])
    if education:
        edu_str = "æ•™è‚²ç»å†ï¼š" + "; ".join([
            f"{e.get('degree', '')} - {e.get('institution', '')} - {e.get('major', '')}"
            for e in education[:3]
        ])
        lines.append(edu_str)
    elif profile_data.get("æ•™è‚²èƒŒæ™¯"):
        lines.append(f"æ•™è‚²èƒŒæ™¯ï¼š{profile_data['æ•™è‚²èƒŒæ™¯']}")
    
    # å·¥ä½œ/ç ”ç©¶ç»å†
    experience = profile_data.get("å·¥ä½œç»å†", [])
    if experience:
        if isinstance(experience, list):
            lines.append(f"å·¥ä½œç»å†ï¼š{'; '.join(experience[:3])}")
        else:
            lines.append(f"å·¥ä½œç»å†ï¼š{experience}")
    
    # æŠ€èƒ½
    skills = profile_data.get("æŠ€èƒ½ç‰¹é•¿", [])
    if skills:
        if isinstance(skills, list):
            lines.append(f"æŠ€èƒ½ç‰¹é•¿ï¼š{', '.join(skills)}")
        else:
            lines.append(f"æŠ€èƒ½ç‰¹é•¿ï¼š{skills}")
    
    # è®ºæ–‡
    publications = profile_data.get("publications", [])
    if publications:
        pub_summary = get_publications_summary(profile_data)
        lines.append(f"è®ºæ–‡å‘è¡¨ï¼šå…± {pub_summary['total']} ç¯‡ (SCI: {pub_summary['sci']}, EI: {pub_summary['ei']})")
    
    # é¡¹ç›®
    grants = profile_data.get("grants", [])
    if grants:
        grant_summary = get_grants_summary(profile_data)
        lines.append(f"ç§‘ç ”é¡¹ç›®ï¼šå…± {grant_summary['total']} é¡¹ï¼Œå…¶ä¸­ä¸»æŒ {grant_summary['as_pi']} é¡¹")
    
    # ä¸ªäººä¼˜åŠ¿
    if profile_data.get("ä¸ªäººä¼˜åŠ¿"):
        lines.append(f"ä¸ªäººä¼˜åŠ¿ï¼š{profile_data['ä¸ªäººä¼˜åŠ¿']}")
    
    # å¯å‘å±•æ–¹å‘
    dev_direction = profile_data.get("å¯å‘å±•æ–¹å‘", {})
    if isinstance(dev_direction, dict) and dev_direction.get("çŸ­æœŸå»ºè®®"):
        lines.append(f"å‘å±•æ–¹å‘ï¼š{dev_direction['çŸ­æœŸå»ºè®®']}")
    
    return "\n".join(lines) if lines else "æš‚æ— è¯¦ç»†ä¿¡æ¯"


# ==================== æ–‡ä»¶å¤„ç† ====================

def extract_excel_content(file) -> Tuple[str, pd.DataFrame]:
    """æå– Excel å†…å®¹ç”¨äº AI åˆ†æ"""
    file.seek(0)
    
    # è¯»å–ä¸º DataFrame
    df = pd.read_excel(file)
    
    # è½¬æ¢ä¸ºæ–‡æœ¬æè¿°
    content_lines = []
    content_lines.append("è¡¨æ ¼åˆ—åï¼š" + ", ".join(df.columns.tolist()))
    
    # å¦‚æœæœ‰æ•°æ®è¡Œï¼Œå±•ç¤ºç»“æ„
    if len(df) > 0:
        content_lines.append("\nè¡¨æ ¼ç»“æ„ç¤ºä¾‹ï¼š")
        for col in df.columns:
            sample = df[col].iloc[0] if pd.notna(df[col].iloc[0]) else "(ç©º)"
            content_lines.append(f"- {col}: {sample}")
    
    return "\n".join(content_lines), df


def extract_word_content(file) -> str:
    """æå– Word å†…å®¹ç”¨äº AI åˆ†æ"""
    if not DOCX_AVAILABLE:
        return "æ— æ³•è¯»å– Word æ–‡ä»¶ï¼ˆæœªå®‰è£… python-docxï¼‰"
    
    file.seek(0)
    doc = Document(file)
    
    content_lines = []
    
    # æå–æ®µè½
    for para in doc.paragraphs:
        if para.text.strip():
            content_lines.append(para.text)
    
    # æå–è¡¨æ ¼
    for table in doc.tables:
        content_lines.append("\n[è¡¨æ ¼å†…å®¹]")
        for row in table.rows:
            row_text = " | ".join([cell.text.strip() for cell in row.cells])
            if row_text.strip():
                content_lines.append(row_text)
    
    return "\n".join(content_lines)


def parse_text_questions(text: str) -> List[str]:
    """è§£æçº¯æ–‡æœ¬é—®é¢˜åˆ—è¡¨"""
    lines = text.strip().split('\n')
    questions = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        # ç§»é™¤å¸¸è§åºå·æ ¼å¼
        line = re.sub(r'^[\d]+[\.ã€\)ï¼‰]\s*', '', line)
        line = re.sub(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€\.]\s*', '', line)
        line = re.sub(r'^[-*â€¢]\s*', '', line)
        
        if line and len(line) > 1:
            questions.append(line)
    
    return questions


# ==================== è¡¨æ ¼æ¨¡å¼æ£€æµ‹ ====================

# ä¸€è¡¨å¤šäººçš„ç‰¹å¾å…³é”®è¯ï¼ˆæ¨ªå‘æ’åˆ—çš„è¡¨å¤´ï¼‰
MULTI_PERSON_KEYWORDS = [
    "åºå·", "ç¼–å·", "No.", "No", "#", "å§“å", "åå­—", "ç”µè¯", "æ‰‹æœº", 
    "é‚®ç®±", "èŒä½", "éƒ¨é—¨", "å·¥å·", "å­¦å·"
]

# ä¸€äººä¸€è¡¨çš„ç‰¹å¾å…³é”®è¯ï¼ˆçºµå‘æ’åˆ—çš„æ ‡ç­¾ï¼‰
SINGLE_PERSON_KEYWORDS = [
    "ç”³è¯·äºº", "å¡«è¡¨äºº", "æœ¬äºº", "ä¸ªäººä¿¡æ¯", "åŸºæœ¬ä¿¡æ¯", "è‡ªæˆ‘ä»‹ç»",
    "ä¸ªäººä¼˜åŠ¿", "èŒä¸šè§„åˆ’", "ç ”ç©¶æ–¹å‘", "é¡¹ç›®ç®€ä»‹"
]


def detect_form_mode_excel(file) -> Tuple[str, str, float]:
    """
    æ£€æµ‹ Excel è¡¨æ ¼çš„å¡«å†™æ¨¡å¼
    
    Returns:
        (mode, reason, confidence)
        mode: "batch" (ä¸€äººä¸€è¡¨) | "aggregate" (ä¸€è¡¨å¤šäºº)
        reason: åˆ¤æ–­ä¾æ®è¯´æ˜
        confidence: ç½®ä¿¡åº¦ 0.0-1.0
    """
    file.seek(0)
    
    try:
        df = pd.read_excel(file)
    except Exception as e:
        return "batch", f"æ— æ³•è§£æè¡¨æ ¼: {str(e)}", 0.5
    
    columns = [str(col).strip() for col in df.columns.tolist()]
    num_columns = len(columns)
    num_rows = len(df)
    
    # ç‰¹å¾åˆ†æ•°
    aggregate_score = 0  # ä¸€è¡¨å¤šäººå¾—åˆ†
    batch_score = 0  # ä¸€äººä¸€è¡¨å¾—åˆ†
    reasons = []
    
    # æ£€æŸ¥1: åˆ—æ•° - å¤šåˆ—é€šå¸¸æ˜¯ä¸€è¡¨å¤šäºº
    if num_columns >= 5:
        aggregate_score += 2
        reasons.append(f"åˆ—æ•°è¾ƒå¤š({num_columns}åˆ—)")
    elif num_columns <= 2:
        batch_score += 2
        reasons.append(f"åˆ—æ•°è¾ƒå°‘({num_columns}åˆ—)ï¼Œåƒæ˜¯æ ‡ç­¾-å€¼ç»“æ„")
    
    # æ£€æŸ¥2: æ˜¯å¦æœ‰åºå·åˆ—
    for col in columns:
        col_lower = col.lower()
        if col_lower in ["åºå·", "ç¼–å·", "no", "no.", "#", "id"]:
            aggregate_score += 3
            reasons.append(f"å­˜åœ¨åºå·åˆ— '{col}'")
            break
    
    # æ£€æŸ¥3: åˆ—åæ˜¯å¦åŒ…å«å¤šäººç‰¹å¾å…³é”®è¯
    multi_keyword_count = 0
    for col in columns:
        for keyword in MULTI_PERSON_KEYWORDS:
            if keyword in col:
                multi_keyword_count += 1
                break
    
    if multi_keyword_count >= 3:
        aggregate_score += 3
        reasons.append(f"åˆ—ååŒ…å«å¤šä¸ªäººå‘˜ä¿¡æ¯å­—æ®µ({multi_keyword_count}ä¸ª)")
    
    # æ£€æŸ¥4: æ˜¯å¦æœ‰ç©ºæ•°æ®è¡Œï¼ˆç­‰å¾…å¡«å†™ï¼‰
    empty_rows = df.isna().all(axis=1).sum()
    if empty_rows >= 2:
        aggregate_score += 2
        reasons.append(f"å­˜åœ¨{empty_rows}ä¸ªç©ºè¡Œå¾…å¡«å†™")
    
    # æ£€æŸ¥5: ç¬¬ä¸€åˆ—æ˜¯å¦åƒæ˜¯æ ‡ç­¾ï¼ˆä¸€äººä¸€è¡¨ç‰¹å¾ï¼‰
    if num_columns == 2:
        first_col_values = df.iloc[:, 0].dropna().astype(str).tolist()
        label_like_count = 0
        for val in first_col_values[:10]:
            # æ£€æŸ¥æ˜¯å¦åƒæ ‡ç­¾ï¼ˆè¾ƒçŸ­ã€åŒ…å«å†’å·æˆ–é—®å·ï¼‰
            if len(val) < 20 and (":" in val or "ï¼š" in val or "?" in val or "ï¼Ÿ" in val):
                label_like_count += 1
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸€äººä¸€è¡¨å…³é”®è¯
            for keyword in SINGLE_PERSON_KEYWORDS:
                if keyword in val:
                    label_like_count += 1
                    break
        
        if label_like_count >= 3:
            batch_score += 3
            reasons.append("ç¬¬ä¸€åˆ—åƒæ˜¯è¡¨å•æ ‡ç­¾")
    
    # æ£€æŸ¥6: è¡Œæ•°åˆ¤æ–­
    if num_rows >= 5 and num_columns >= 3:
        aggregate_score += 1
        reasons.append(f"è¡¨æ ¼æœ‰{num_rows}è¡Œï¼Œé€‚åˆå¡«å…¥å¤šäºº")
    elif num_rows >= 10 and num_columns == 2:
        batch_score += 1
        reasons.append(f"çºµå‘ç»“æ„ï¼Œ{num_rows}ä¸ªå­—æ®µ")
    
    # è®¡ç®—æœ€ç»ˆç»“æœ
    total_score = aggregate_score + batch_score
    if total_score == 0:
        return "batch", "æ— æ³•ç¡®å®šï¼Œé»˜è®¤ä½¿ç”¨ä¸€äººä¸€è¡¨", 0.5
    
    if aggregate_score > batch_score:
        confidence = min(0.95, 0.5 + (aggregate_score - batch_score) * 0.1)
        mode = "aggregate"
        mode_desc = "ä¸€è¡¨å¤šäºº"
    else:
        confidence = min(0.95, 0.5 + (batch_score - aggregate_score) * 0.1)
        mode = "batch"
        mode_desc = "ä¸€äººä¸€è¡¨"
    
    reason_text = f"åˆ¤æ–­ä¸º{mode_desc}ï¼š" + "ï¼›".join(reasons[:3])
    return mode, reason_text, confidence


def detect_form_mode_word(file) -> Tuple[str, str, float]:
    """
    æ£€æµ‹ Word æ–‡æ¡£çš„å¡«å†™æ¨¡å¼
    
    Returns:
        (mode, reason, confidence)
    """
    if not DOCX_AVAILABLE:
        return "batch", "æ— æ³•è§£æ Word æ–‡ä»¶", 0.5
    
    file.seek(0)
    
    try:
        doc = Document(file)
    except Exception as e:
        return "batch", f"æ— æ³•è§£ææ–‡æ¡£: {str(e)}", 0.5
    
    aggregate_score = 0
    batch_score = 0
    reasons = []
    
    # æ£€æŸ¥è¡¨æ ¼
    tables = doc.tables
    if tables:
        for table in tables:
            num_rows = len(table.rows)
            num_cols = len(table.columns) if table.rows else 0
            
            # å¤šè¡Œå¤šåˆ—çš„è¡¨æ ¼ -> ä¸€è¡¨å¤šäºº
            if num_rows >= 3 and num_cols >= 3:
                aggregate_score += 3
                reasons.append(f"åŒ…å« {num_rows}x{num_cols} çš„è¡¨æ ¼")
            
            # ä¸¤åˆ—è¡¨æ ¼ -> å¯èƒ½æ˜¯ä¸€äººä¸€è¡¨
            elif num_cols == 2 and num_rows >= 5:
                batch_score += 2
                reasons.append("åŒ…å«ä¸¤åˆ—è¡¨æ ¼ï¼ˆæ ‡ç­¾-å€¼ç»“æ„ï¼‰")
            
            # æ£€æŸ¥è¡¨å¤´
            if table.rows:
                header_cells = [cell.text.strip() for cell in table.rows[0].cells]
                header_text = " ".join(header_cells)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¤šäººç‰¹å¾å…³é”®è¯
                multi_count = sum(1 for kw in MULTI_PERSON_KEYWORDS if kw in header_text)
                if multi_count >= 2:
                    aggregate_score += 2
                    reasons.append("è¡¨å¤´åŒ…å«å¤šäººä¿¡æ¯å­—æ®µ")
    else:
        # æ²¡æœ‰è¡¨æ ¼ï¼Œæ£€æŸ¥æ®µè½
        batch_score += 1
        reasons.append("æ— è¡¨æ ¼ï¼Œå¯èƒ½æ˜¯æ–‡æœ¬å‹è¡¨å•")
    
    # æ£€æŸ¥æ®µè½ä¸­çš„å ä½ç¬¦
    placeholder_count = 0
    for para in doc.paragraphs:
        text = para.text
        # æ£€æŸ¥ {{xxx}} å ä½ç¬¦
        placeholders = re.findall(r'\{\{[^}]+\}\}', text)
        placeholder_count += len(placeholders)
    
    if placeholder_count > 0:
        batch_score += 2
        reasons.append(f"åŒ…å« {placeholder_count} ä¸ªå ä½ç¬¦")
    
    # è®¡ç®—ç»“æœ
    total_score = aggregate_score + batch_score
    if total_score == 0:
        return "batch", "æ— æ³•ç¡®å®šï¼Œé»˜è®¤ä½¿ç”¨ä¸€äººä¸€è¡¨", 0.5
    
    if aggregate_score > batch_score:
        confidence = min(0.95, 0.5 + (aggregate_score - batch_score) * 0.1)
        mode = "aggregate"
        mode_desc = "ä¸€è¡¨å¤šäºº"
    else:
        confidence = min(0.95, 0.5 + (batch_score - aggregate_score) * 0.1)
        mode = "batch"
        mode_desc = "ä¸€äººä¸€è¡¨"
    
    reason_text = f"åˆ¤æ–­ä¸º{mode_desc}ï¼š" + "ï¼›".join(reasons[:3]) if reasons else f"åˆ¤æ–­ä¸º{mode_desc}"
    return mode, reason_text, confidence


def detect_form_mode(file, file_type: str) -> Tuple[str, str, float]:
    """
    æ£€æµ‹è¡¨æ ¼å¡«å†™æ¨¡å¼ï¼ˆç»Ÿä¸€å…¥å£ï¼‰
    
    Args:
        file: ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
        file_type: "excel" | "word"
    
    Returns:
        (mode, reason, confidence)
        mode: "batch" (ä¸€äººä¸€è¡¨) | "aggregate" (ä¸€è¡¨å¤šäºº)
        reason: åˆ¤æ–­ä¾æ®è¯´æ˜
        confidence: ç½®ä¿¡åº¦ 0.0-1.0
    """
    if file_type == "excel":
        return detect_form_mode_excel(file)
    elif file_type == "word":
        return detect_form_mode_word(file)
    else:
        return "batch", "æœªçŸ¥æ–‡ä»¶ç±»å‹ï¼Œé»˜è®¤ä½¿ç”¨ä¸€äººä¸€è¡¨", 0.5


# ==================== å¡«å†™å’Œå¯¼å‡º ====================

def fill_excel_with_answers(template_file, answers: Dict[str, str]) -> bytes:
    """å°†ç­”æ¡ˆå¡«å…¥ Excel å¹¶è¿”å›"""
    template_file.seek(0)
    wb = load_workbook(template_file)
    ws = wb.active
    
    # æŸ¥æ‰¾å¹¶å¡«å†™
    for row in ws.iter_rows():
        for cell in row:
            if cell.value and isinstance(cell.value, str):
                cell_text = cell.value.strip()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯å­—æ®µå
                for field, answer in answers.items():
                    if field in cell_text or cell_text in field:
                        # æ‰¾åˆ°ä¸‹ä¸€ä¸ªå•å…ƒæ ¼æˆ–åŒè¡Œå³ä¾§å•å…ƒæ ¼å¡«å†™ç­”æ¡ˆ
                        next_cell = ws.cell(row=cell.row, column=cell.column + 1)
                        if not next_cell.value:
                            next_cell.value = answer
                            next_cell.alignment = Alignment(wrap_text=True)
    
    output = io.BytesIO()
    wb.save(output)
    output.seek(0)
    return output.getvalue()


def fill_word_with_answers(template_file, answers: Dict[str, str]) -> bytes:
    """å°†ç­”æ¡ˆå¡«å…¥ Word å¹¶è¿”å›"""
    if not DOCX_AVAILABLE:
        raise ImportError("python-docx æœªå®‰è£…")
    
    template_file.seek(0)
    doc = Document(template_file)
    
    # å¡«å†™æ®µè½ä¸­çš„å­—æ®µ
    for para in doc.paragraphs:
        for field, answer in answers.items():
            if field in para.text:
                # åœ¨å­—æ®µåæ·»åŠ ç­”æ¡ˆ
                para.text = para.text.replace(field, f"{field}ï¼š{answer}")
    
    # å¡«å†™è¡¨æ ¼
    for table in doc.tables:
        for row in table.rows:
            cells = row.cells
            for i, cell in enumerate(cells):
                cell_text = cell.text.strip()
                if cell_text in answers:
                    # å¦‚æœæœ‰ä¸‹ä¸€ä¸ªå•å…ƒæ ¼ï¼Œå¡«å…¥ç­”æ¡ˆ
                    if i + 1 < len(cells):
                        cells[i + 1].text = answers[cell_text]
    
    output = io.BytesIO()
    doc.save(output)
    output.seek(0)
    return output.getvalue()


def export_answers_to_csv(answers: Dict[str, str]) -> str:
    """å¯¼å‡ºç­”æ¡ˆä¸º CSV"""
    df = pd.DataFrame([
        {"å­—æ®µ": k, "ç­”æ¡ˆ": v} for k, v in answers.items()
    ])
    return df.to_csv(index=False, encoding='utf-8-sig')


# ==================== Streamlit UI ====================

def render_smart_form_filler(api_key: str):
    """æ¸²æŸ“æ™ºèƒ½å¡«è¡¨ç•Œé¢"""
    
    st.header("ğŸª„ æ™ºèƒ½å¡«è¡¨")
    st.markdown("ä¸Šä¼ ç©ºè¡¨æ ¼ï¼ŒAI è‡ªåŠ¨è¯†åˆ«å­—æ®µå¹¶å¡«å†™ï¼Œæ”¯æŒæ¶¦è‰²è¯æœ¯")
    
    if not api_key:
        st.warning("âš ï¸ è¯·å…ˆåœ¨ä¾§è¾¹æ è¾“å…¥ DeepSeek API Key")
        return
    
    st.markdown("---")
    
    # é€‰æ‹©è¾“å…¥æ–¹å¼
    input_method = st.radio(
        "é€‰æ‹©è¾“å…¥æ–¹å¼",
        ["ğŸ“„ ä¸Šä¼  Excel è¡¨æ ¼", "ğŸ“ ä¸Šä¼  Word æ–‡æ¡£", "âœï¸ ç²˜è´´é—®é¢˜åˆ—è¡¨"],
        horizontal=True
    )
    
    # é€‰æ‹©ç”¨æˆ·æ¡£æ¡ˆï¼ˆæ ¹æ®å½“å‰ç”¨æˆ·æ¨¡å¼è¿‡æ»¤ï¼‰
    st.markdown("---")
    st.subheader("ğŸ‘¤ é€‰æ‹©è¦ä½¿ç”¨çš„æ¡£æ¡ˆ")
    
    from self_config import load_self_profile_from_orgstore, get_self_person_id
    from store_org import load_profiles_multi, load_groups, load_people
    
    # è·å–å½“å‰ç”¨æˆ·æ¨¡å¼
    current_mode = st.session_state.get('mode', 'single')
    
    all_profiles = []
    
    if current_mode == 'single':
        # ä¸ªäººç‰ˆï¼šä» OrgStore åŠ è½½ã€Œæˆ‘è‡ªå·±ã€çš„æ¡£æ¡ˆï¼ˆæ–°æ–¹å¼ï¼šä¸ªäººç‰ˆ=å¤šäººç‰ˆä¸­çš„ä¸€å‘˜ï¼‰
        self_data = load_self_profile_from_orgstore()
        if self_data:
            profile_data = self_data.get("profile", {})
            person_id = self_data.get("person_id", "single")
            if isinstance(profile_data, dict):
                all_profiles.append({
                    "id": person_id,
                    "name": self_data.get("name") or profile_data.get("å§“å", "ä¸ªäººç‰ˆç”¨æˆ·"),
                    "source": "ä¸ªäººæ¡£æ¡ˆ",
                    "data": profile_data
                })
        else:
            # æœªç»‘å®šã€Œæˆ‘è‡ªå·±ã€ï¼Œæç¤ºç”¨æˆ·
            st.info("ğŸ’¡ è¯·å…ˆåœ¨ã€Œæ•°æ®ç®¡ç†ã€ä¸­è®¾ç½®ä¸ªäººä¿¡æ¯æˆ–ç»‘å®šã€Œæˆ‘æ˜¯è°ã€")
    else:
        # å¤šäººç‰ˆï¼šåŠ è½½å¤šäººç‰ˆæ¡£æ¡ˆ
        multi_profiles = load_profiles_multi()
        for mp in multi_profiles:
            profile_data = mp.get("profile", {})
            if isinstance(profile_data, dict):
                all_profiles.append({
                    "id": mp["id"],
                    "name": profile_data.get("å§“å", mp.get("name", "æœªçŸ¥")),
                    "source": "å›¢é˜Ÿæˆå‘˜",
                    "data": profile_data,
                    "groups": mp.get("groups", []),  # ä¿å­˜æ‰€å±å›¢é˜Ÿä¿¡æ¯
                    "memberships": mp.get("memberships", [])
                })
    
    # åŒæ—¶åŠ è½½ç ”ç©¶æ¡£æ¡ˆï¼ˆä¸¤ç§æ¨¡å¼éƒ½å¯ç”¨ï¼‰
    research_profiles = load_research_profiles()
    for p in research_profiles:
        all_profiles.append({
            "id": p["id"],
            "name": p.get("å§“å", "æœªçŸ¥"),
            "source": "ç§‘ç ”æ¡£æ¡ˆ",
            "data": p
        })
    
    if not all_profiles:
        mode_name = "ä¸ªäººç‰ˆ" if current_mode == 'single' else "å¤šäººç‰ˆ"
        st.warning(f"æ²¡æœ‰å¯ç”¨çš„æ¡£æ¡ˆï¼Œè¯·å…ˆåœ¨ã€Œæ•°æ®ç®¡ç†ã€ä¸­æ·»åŠ {mode_name}ç”¨æˆ·ä¿¡æ¯")
        return
    
    profile_options = {p["id"]: f"{p['name']} ({p['source']})" for p in all_profiles}
    
    # å¤šäººç‰ˆæ”¯æŒå¤šé€‰ï¼šæŒ‰äººå‘˜é€‰æ‹© æˆ– æŒ‰å›¢é˜Ÿé€‰æ‹©
    if current_mode == 'multi' and len(all_profiles) > 1:
        # è·å–æ‰€æœ‰å›¢é˜Ÿ
        groups = load_groups()
        
        # é€‰æ‹©æ–¹å¼
        selection_mode = st.radio(
            "é€‰æ‹©æ–¹å¼",
            ["ğŸ‘¤ æŒ‰äººå‘˜é€‰æ‹©", "ğŸ‘¥ æŒ‰å›¢é˜Ÿé€‰æ‹©"],
            horizontal=True,
            key="profile_selection_mode"
        )
        
        if "æŒ‰å›¢é˜Ÿé€‰æ‹©" in selection_mode and groups:
            # æŒ‰å›¢é˜Ÿé€‰æ‹©
            st.info("ğŸ’¡ é€‰æ‹©å›¢é˜Ÿåï¼Œå°†è‡ªåŠ¨é€‰ä¸­è¯¥å›¢é˜Ÿçš„æ‰€æœ‰æˆå‘˜")
            
            group_options = {g["id"]: g["name"] for g in groups}
            selected_group_ids = st.multiselect(
                "é€‰æ‹©å›¢é˜Ÿï¼ˆå¯å¤šé€‰ï¼‰",
                options=list(group_options.keys()),
                format_func=lambda x: f"ğŸ“ {group_options[x]}",
                key="select_groups_for_form"
            )
            
            # æ ¹æ®é€‰ä¸­çš„å›¢é˜Ÿç­›é€‰æˆå‘˜
            if selected_group_ids:
                selected_profiles = []
                for p in all_profiles:
                    # æ£€æŸ¥è¯¥æˆå‘˜æ˜¯å¦å±äºé€‰ä¸­çš„ä»»ä¸€å›¢é˜Ÿ
                    memberships = p.get("memberships", [])
                    for ms in memberships:
                        if ms.get("group_id") in selected_group_ids:
                            if p not in selected_profiles:
                                selected_profiles.append(p)
                            break
                
                # æ˜¾ç¤ºå·²é€‰å›¢é˜Ÿçš„æˆå‘˜æ•°
                if selected_profiles:
                    group_names = [group_options.get(gid, gid) for gid in selected_group_ids]
                    st.success(f"å·²é€‰æ‹©å›¢é˜Ÿï¼š{', '.join(group_names)}ï¼Œå…± {len(selected_profiles)} äºº")
                else:
                    st.warning("é€‰ä¸­çš„å›¢é˜Ÿæš‚æ— æˆå‘˜")
                    selected_profiles = []
            else:
                selected_profiles = []
                st.info("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªå›¢é˜Ÿ")
        else:
            # æŒ‰äººå‘˜é€‰æ‹©
            if "æŒ‰å›¢é˜Ÿé€‰æ‹©" in selection_mode and not groups:
                st.info("ğŸ’¡ æš‚æ— å›¢é˜Ÿæ•°æ®ï¼Œè¯·å…ˆåœ¨ã€Œæ•°æ®ç®¡ç†ã€ä¸­åˆ›å»ºå›¢é˜Ÿ")
            
            st.info("ğŸ’¡ å¤šäººç‰ˆæ¨¡å¼ï¼šå¯é€‰æ‹©å¤šä¸ªäººå‘˜æ‰¹é‡ç”Ÿæˆ")
            selected_profile_ids = st.multiselect(
                "é€‰æ‹©æ¡£æ¡ˆï¼ˆå¯å¤šé€‰ï¼‰",
                options=list(profile_options.keys()),
                default=[list(profile_options.keys())[0]] if profile_options else [],
                format_func=lambda x: profile_options[x],
                key="select_profiles_for_form"
            )
            selected_profiles = [p for p in all_profiles if p["id"] in selected_profile_ids]
    else:
        selected_profile_id = st.selectbox(
            "é€‰æ‹©æ¡£æ¡ˆ",
            options=list(profile_options.keys()),
            format_func=lambda x: profile_options[x]
        )
        selected_profiles = [p for p in all_profiles if p["id"] == selected_profile_id]
    
    if not selected_profiles:
        st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ¡£æ¡ˆ")
        return
    
    # æ˜¾ç¤ºå·²é€‰æ¡£æ¡ˆ
    if len(selected_profiles) == 1:
        with st.expander("æŸ¥çœ‹æ¡£æ¡ˆä¿¡æ¯"):
            st.json(selected_profiles[0]["data"])
    else:
        with st.expander(f"æŸ¥çœ‹å·²é€‰ {len(selected_profiles)} äººçš„æ¡£æ¡ˆ"):
            for p in selected_profiles:
                st.markdown(f"**{p['name']}** ({p['source']})")
                st.json(p["data"])
                st.markdown("---")
    
    st.markdown("---")
    
    # æ ¹æ®è¾“å…¥æ–¹å¼å¤„ç†
    if "Excel" in input_method:
        _handle_excel_input(api_key, selected_profiles, all_profiles)
    elif "Word" in input_method:
        _handle_word_input(api_key, selected_profiles, all_profiles)
    else:
        _handle_text_input(api_key, selected_profiles)


def _handle_excel_input(api_key: str, selected_profiles: List[Dict], all_profiles: List[Dict]):
    """å¤„ç† Excel è¾“å…¥ï¼ˆæ”¯æŒæ¨¡å¼æ£€æµ‹å’Œå¤šäººæ‰¹é‡ï¼‰"""
    
    uploaded_file = st.file_uploader(
        "ä¸Šä¼  Excel è¡¨æ ¼",
        type=['xlsx', 'xls'],
        help="ä¸Šä¼ éœ€è¦å¡«å†™çš„ç©ºç™½è¡¨æ ¼"
    )
    
    if not uploaded_file:
        st.info("è¯·ä¸Šä¼  Excel æ–‡ä»¶")
        return
    
    # æå–å†…å®¹
    content, df = extract_excel_content(uploaded_file)
    
    st.markdown("**è¡¨æ ¼é¢„è§ˆï¼š**")
    st.dataframe(df, use_container_width=True)
    
    # è‡ªåŠ¨æ£€æµ‹è¡¨æ ¼æ¨¡å¼
    st.markdown("---")
    st.subheader("ğŸ“Š è¡¨æ ¼æ¨¡å¼æ£€æµ‹")
    
    detected_mode, reason, confidence = detect_form_mode(uploaded_file, "excel")
    
    # æ˜¾ç¤ºæ£€æµ‹ç»“æœ
    mode_labels = {"batch": "ä¸€äººä¸€è¡¨", "aggregate": "ä¸€è¡¨å¤šäºº"}
    confidence_pct = int(confidence * 100)
    
    st.markdown(f"**AI åˆ¤æ–­ç»“æœï¼š** {mode_labels[detected_mode]} (ç½®ä¿¡åº¦: {confidence_pct}%)")
    st.caption(reason)
    
    # æ‰‹åŠ¨åˆ‡æ¢é€‰é¡¹
    form_mode = st.radio(
        "é€‰æ‹©å¡«å†™æ¨¡å¼",
        ["batch", "aggregate"],
        index=0 if detected_mode == "batch" else 1,
        format_func=lambda x: "ğŸ“‚ ä¸€äººä¸€è¡¨ (æ¯äººç”Ÿæˆä¸€ä¸ªæ–‡ä»¶)" if x == "batch" else "ğŸ“‘ ä¸€è¡¨å¤šäºº (æ‰€æœ‰äººå¡«å…¥åŒä¸€è¡¨æ ¼)",
        horizontal=True,
        help="å¦‚æœ AI åˆ¤æ–­ä¸å‡†ç¡®ï¼Œå¯ä»¥æ‰‹åŠ¨åˆ‡æ¢"
    )
    
    if form_mode != detected_mode:
        st.info("å·²åˆ‡æ¢ä¸ºæ‰‹åŠ¨é€‰æ‹©æ¨¡å¼")
    
    # æ ¹æ®æ¨¡å¼æ˜¾ç¤ºä¸åŒæç¤º
    if form_mode == "batch":
        st.success(f"ğŸ“‚ å°†ä¸º {len(selected_profiles)} äººå„ç”Ÿæˆä¸€ä»½å¡«å†™ç»“æœ")
    else:
        st.success(f"ğŸ“‘ å°†æŠŠ {len(selected_profiles)} äººçš„ä¿¡æ¯å¡«å…¥åŒä¸€è¡¨æ ¼")
    
    st.markdown("---")
    
    # AI è¯†åˆ«å­—æ®µ
    if st.button("ğŸ” AI è¯†åˆ«å­—æ®µ", type="primary"):
        with st.spinner("AI æ­£åœ¨åˆ†æè¡¨æ ¼ç»“æ„..."):
            client = create_ai_client(api_key)
            fields = ai_identify_fields(client, content)
        
        if fields:
            st.session_state['identified_fields'] = fields
            st.session_state['uploaded_file'] = uploaded_file
            st.session_state['form_mode'] = form_mode
            st.success(f"âœ… è¯†åˆ«åˆ° {len(fields)} ä¸ªå­—æ®µ")
    
    # æ˜¾ç¤ºè¯†åˆ«ç»“æœå¹¶ç”Ÿæˆç­”æ¡ˆ
    if 'identified_fields' in st.session_state:
        current_form_mode = st.session_state.get('form_mode', form_mode)
        _render_field_filling_multi(api_key, selected_profiles, st.session_state['identified_fields'], "excel", current_form_mode)


def _handle_word_input(api_key: str, selected_profiles: List[Dict], all_profiles: List[Dict]):
    """å¤„ç† Word è¾“å…¥ï¼ˆæ”¯æŒæ¨¡å¼æ£€æµ‹å’Œå¤šäººæ‰¹é‡ï¼‰"""
    
    if not DOCX_AVAILABLE:
        st.error("æœªå®‰è£… python-docxï¼Œæ— æ³•å¤„ç† Word æ–‡ä»¶")
        return
    
    uploaded_file = st.file_uploader(
        "ä¸Šä¼  Word æ–‡æ¡£",
        type=['docx'],
        help="ä¸Šä¼ éœ€è¦å¡«å†™çš„æ–‡æ¡£æ¨¡æ¿"
    )
    
    if not uploaded_file:
        st.info("è¯·ä¸Šä¼  Word æ–‡ä»¶")
        return
    
    # æå–å†…å®¹
    content = extract_word_content(uploaded_file)
    
    with st.expander("æ–‡æ¡£å†…å®¹é¢„è§ˆ"):
        st.text(content[:2000] + ("..." if len(content) > 2000 else ""))
    
    # è‡ªåŠ¨æ£€æµ‹æ–‡æ¡£æ¨¡å¼
    st.markdown("---")
    st.subheader("ğŸ“Š æ–‡æ¡£æ¨¡å¼æ£€æµ‹")
    
    detected_mode, reason, confidence = detect_form_mode(uploaded_file, "word")
    
    # æ˜¾ç¤ºæ£€æµ‹ç»“æœ
    mode_labels = {"batch": "ä¸€äººä¸€è¡¨", "aggregate": "ä¸€è¡¨å¤šäºº"}
    confidence_pct = int(confidence * 100)
    
    st.markdown(f"**AI åˆ¤æ–­ç»“æœï¼š** {mode_labels[detected_mode]} (ç½®ä¿¡åº¦: {confidence_pct}%)")
    st.caption(reason)
    
    # æ‰‹åŠ¨åˆ‡æ¢é€‰é¡¹
    form_mode = st.radio(
        "é€‰æ‹©å¡«å†™æ¨¡å¼",
        ["batch", "aggregate"],
        index=0 if detected_mode == "batch" else 1,
        format_func=lambda x: "ğŸ“‚ ä¸€äººä¸€è¡¨ (æ¯äººç”Ÿæˆä¸€ä¸ªæ–‡ä»¶)" if x == "batch" else "ğŸ“‘ ä¸€è¡¨å¤šäºº (æ‰€æœ‰äººå¡«å…¥åŒä¸€æ–‡æ¡£)",
        horizontal=True,
        help="å¦‚æœ AI åˆ¤æ–­ä¸å‡†ç¡®ï¼Œå¯ä»¥æ‰‹åŠ¨åˆ‡æ¢",
        key="word_form_mode"
    )
    
    if form_mode != detected_mode:
        st.info("å·²åˆ‡æ¢ä¸ºæ‰‹åŠ¨é€‰æ‹©æ¨¡å¼")
    
    # æ ¹æ®æ¨¡å¼æ˜¾ç¤ºä¸åŒæç¤º
    if form_mode == "batch":
        st.success(f"ğŸ“‚ å°†ä¸º {len(selected_profiles)} äººå„ç”Ÿæˆä¸€ä»½å¡«å†™ç»“æœ")
    else:
        st.success(f"ğŸ“‘ å°†æŠŠ {len(selected_profiles)} äººçš„ä¿¡æ¯å¡«å…¥åŒä¸€æ–‡æ¡£")
    
    st.markdown("---")
    
    # AI è¯†åˆ«å­—æ®µ
    if st.button("ğŸ” AI è¯†åˆ«å­—æ®µ", type="primary", key="word_identify"):
        with st.spinner("AI æ­£åœ¨åˆ†ææ–‡æ¡£ç»“æ„..."):
            client = create_ai_client(api_key)
            fields = ai_identify_fields(client, content)
        
        if fields:
            st.session_state['identified_fields'] = fields
            st.session_state['uploaded_file'] = uploaded_file
            st.session_state['form_mode'] = form_mode
            st.success(f"âœ… è¯†åˆ«åˆ° {len(fields)} ä¸ªå­—æ®µ")
    
    if 'identified_fields' in st.session_state:
        current_form_mode = st.session_state.get('form_mode', form_mode)
        _render_field_filling_multi(api_key, selected_profiles, st.session_state['identified_fields'], "word", current_form_mode)


def _handle_text_input(api_key: str, profiles: List[Dict]):
    """å¤„ç†çº¯æ–‡æœ¬é—®é¢˜è¾“å…¥ï¼ˆæ”¯æŒå•äºº/å¤šäººï¼‰"""
    if not profiles:
        st.warning("è¯·å…ˆé€‰æ‹©ä¸€ä¸ªæ¡£æ¡ˆ")
        return
    
    questions_text = st.text_area(
        "ç²˜è´´é—®é¢˜åˆ—è¡¨",
        height=200,
        key="text_questions_input",
        placeholder="ç¤ºä¾‹ï¼š\n1. è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±\n2. ä½ çš„ç ”ç©¶æ–¹å‘æ˜¯ä»€ä¹ˆ\n3. ä¸ºä»€ä¹ˆé€‰æ‹©æˆ‘ä»¬å­¦æ ¡"
    )
    
    if not questions_text.strip():
        st.info("è¯·è¾“å…¥é—®é¢˜åˆ—è¡¨")
        return
    
    questions = parse_text_questions(questions_text)
    if not questions:
        st.warning("æœªè¯†åˆ«åˆ°æœ‰æ•ˆé—®é¢˜ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ ¼å¼ï¼ˆæ”¯æŒæ¢è¡Œ/åºå·/é¡¹ç›®ç¬¦å·ï¼‰")
        return
    
    st.markdown(f"**è¯†åˆ«åˆ° {len(questions)} ä¸ªé—®é¢˜ï¼š**")
    for i, q in enumerate(questions, 1):
        st.markdown(f"{i}. {q}")
    
    # è½¬æ¢ä¸ºå­—æ®µæ ¼å¼
    fields = [{"field": q, "type": "subjective", "description": "é—®ç­”é¢˜"} for q in questions]

    st.markdown("---")
    st.subheader("ğŸ“ ç”Ÿæˆä¸ç¼–è¾‘")

    # å¤šäººï¼šæ”¯æŒâ€œæ¯äººä¸€ä»½ / æ±‡æ€»å¯¹ç…§è¡¨â€
    if len(profiles) > 1:
        form_mode = st.radio(
            "è¾“å‡ºæ–¹å¼",
            ["batch", "aggregate"],
            index=0,
            format_func=lambda x: "ğŸ‘¤ æ¯äººä¸€ä»½ï¼ˆæ¨èï¼‰" if x == "batch" else "ğŸ“Š æ±‡æ€»å¯¹ç…§è¡¨ï¼ˆä¾¿äºæ¨ªå‘æ¯”è¾ƒï¼‰",
            horizontal=True,
            key="text_output_mode",
        )
        _render_field_filling_multi(api_key, profiles, fields, "text", form_mode)
    else:
        _render_field_filling(api_key, profiles[0], fields, "text")


def _render_field_filling(api_key: str, profile: Dict, fields: List[Dict], file_type: str):
    """æ¸²æŸ“å­—æ®µå¡«å†™ç•Œé¢"""
    
    st.markdown("---")
    st.subheader("ğŸ“ å­—æ®µå¡«å†™")

    # ========== ä¿¡æ¯å®Œæ•´æ€§æ ¡éªŒï¼šä¸å®Œæ•´åˆ™ç¦æ­¢ç”Ÿæˆ/å¯¼å‡º ==========
    validation = None
    try:
        source_label = str(profile.get("source", ""))
        if "ç§‘ç ”" in source_label:
            validation = validate_research_profile(profile.get("data"))
        else:
            validation = validate_general_profile(profile.get("data"))
    except Exception:
        validation = {"is_complete": False, "missing_required": ["ç”»åƒç»“æ„å¼‚å¸¸"], "issues": ["æ ¡éªŒå™¨å¼‚å¸¸"]}

    can_output = bool(validation and validation.get("is_complete"))

    if not can_output:
        missing = validation.get("missing_required", []) if isinstance(validation, dict) else []
        st.error("âš ï¸ ä¸ªäººå¿…å¡«ä¿¡æ¯ä¸å®Œæ•´ï¼šæ— æ³•ç”Ÿæˆ/å¯¼å‡ºè¡¨æ ¼ç»“æœã€‚"
                 + (f" ç¼ºå°‘ï¼š{'ã€'.join(missing)}" if missing else ""))
        with st.expander("æŸ¥çœ‹å¿…å¡«é¡¹è¯¦æƒ…"):
            items_required = validation.get("items_required", []) if isinstance(validation, dict) else []
            for it in items_required:
                ok = it.get("ok", False)
                label = it.get("label", "")
                st.markdown(("âœ… " if ok else "âŒ ") + f"**{label}**")
    
    # åˆå§‹åŒ–ç­”æ¡ˆå­˜å‚¨
    if 'generated_answers' not in st.session_state:
        st.session_state['generated_answers'] = {}
    
    # æ˜¾ç¤ºå­—æ®µåˆ—è¡¨
    answers = st.session_state['generated_answers']
    
    # æ¶¦è‰²é£æ ¼é€‰æ‹©
    polish_style = st.selectbox(
        "æ¶¦è‰²é£æ ¼",
        ["professional", "academic", "friendly"],
        format_func=lambda x: {"professional": "ä¸“ä¸šæ­£å¼", "academic": "å­¦æœ¯è§„èŒƒ", "friendly": "äº²å’Œå‹å¥½"}[x]
    )
    
    # æ‰¹é‡ç”ŸæˆæŒ‰é’®
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸš€ æ‰¹é‡ç”Ÿæˆæ‰€æœ‰ç­”æ¡ˆ", use_container_width=True, disabled=not can_output):
            client = create_ai_client(api_key)
            
            with st.spinner("AI æ­£åœ¨æ‰¹é‡ç”Ÿæˆæ‰€æœ‰ç­”æ¡ˆ...ï¼ˆä¸€æ¬¡è°ƒç”¨ï¼Œé€Ÿåº¦æ›´å¿«ï¼‰"):
                # ä½¿ç”¨æ‰¹é‡ç”Ÿæˆï¼ˆä¸€æ¬¡ API è°ƒç”¨ï¼‰
                batch_answers = ai_batch_generate_answers(client, fields, profile["data"], polish_style)
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
            if "_error" in batch_answers:
                st.error(batch_answers["_error"])
            elif batch_answers:
                # æ‰¹é‡ç”ŸæˆæˆåŠŸ
                for field_info in fields:
                    field = field_info["field"]
                    if field in batch_answers:
                        answers[field] = batch_answers[field]
                
                st.session_state['generated_answers'] = answers
                st.success(f"âœ… å·²æ‰¹é‡ç”Ÿæˆ {len(batch_answers)} ä¸ªå­—æ®µçš„ç­”æ¡ˆï¼")
                st.rerun()
            else:
                # æ‰¹é‡ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°é€ä¸ªç”Ÿæˆ
                st.warning("æ‰¹é‡ç”Ÿæˆå¤±è´¥ï¼Œæ­£åœ¨é€ä¸ªç”Ÿæˆ...")
                progress = st.progress(0)
                
                for i, field_info in enumerate(fields):
                    field = field_info["field"]
                    field_type = field_info.get("type", "subjective")
                    
                    with st.spinner(f"ç”Ÿæˆ: {field}..."):
                        answer = ai_generate_answer(client, field, field_type, profile["data"])
                        answers[field] = answer
                    
                    progress.progress((i + 1) / len(fields))
                
                st.session_state['generated_answers'] = answers
                st.success("âœ… æ‰€æœ‰ç­”æ¡ˆå·²ç”Ÿæˆï¼")
                st.rerun()
    
    with col2:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰ç­”æ¡ˆ", use_container_width=True):
            st.session_state['generated_answers'] = {}
            st.rerun()
    
    st.markdown("---")
    
    # æ˜¾ç¤ºæ¯ä¸ªå­—æ®µçš„ç­”æ¡ˆï¼ˆå¯ç¼–è¾‘ï¼‰
    for field_info in fields:
        field = field_info["field"]
        field_type = field_info.get("type", "subjective")
        desc = field_info.get("description", "")
        
        type_badge = "ğŸ”µ äº‹å®ç±»" if field_type == "factual" else "ğŸŸ¢ ä¸»è§‚ç±»"
        
        st.markdown(f"**{field}** {type_badge}")
        if desc:
            st.caption(desc)
        
        # å¯ç¼–è¾‘çš„ç­”æ¡ˆ
        current_answer = answers.get(field, "")
        new_answer = st.text_area(
            f"ç­”æ¡ˆ",
            value=current_answer,
            key=f"answer_{field}",
            height=100 if field_type == "subjective" else 50,
            label_visibility="collapsed"
        )
        
        # æ›´æ–°ç­”æ¡ˆ
        if new_answer != current_answer:
            answers[field] = new_answer
            st.session_state['generated_answers'] = answers
        
        # å•ç‹¬ç”Ÿæˆ/æ¶¦è‰²æŒ‰é’®
        col_a, col_b = st.columns([1, 1])
        with col_a:
            if st.button(f"ç”Ÿæˆ", key=f"gen_{field}", disabled=not can_output):
                client = create_ai_client(api_key)
                with st.spinner("ç”Ÿæˆä¸­..."):
                    answer = ai_generate_answer(client, field, field_type, profile["data"])
                    if field_type == "subjective":
                        answer = ai_polish_text(client, answer, polish_style)
                    answers[field] = answer
                    st.session_state['generated_answers'] = answers
                st.rerun()
        
        with col_b:
            if new_answer and st.button(f"æ¶¦è‰²", key=f"polish_{field}", disabled=not can_output):
                client = create_ai_client(api_key)
                with st.spinner("æ¶¦è‰²ä¸­..."):
                    polished = ai_polish_text(client, new_answer, polish_style)
                    answers[field] = polished
                    st.session_state['generated_answers'] = answers
                st.rerun()
        
        st.markdown("---")
    
    # å¯¼å‡ºæŒ‰é’®
    if answers:
        st.subheader("ğŸ“¤ å¯¼å‡ºç»“æœ")

        if not can_output:
            st.warning("ä¿¡æ¯ä¸å®Œæ•´ï¼šå¯¼å‡ºå·²ç¦ç”¨ã€‚è¯·å…ˆè¡¥å…¨å¿…å¡«ä¿¡æ¯ã€‚")
            return
        
        col_exp1, col_exp2 = st.columns(2)
        
        with col_exp1:
            # å¯¼å‡ºä¸º CSV
            csv_data = export_answers_to_csv(answers)
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½ CSV",
                data=csv_data,
                file_name="filled_form.csv",
                mime="text/csv",
                use_container_width=True
            )
        
        with col_exp2:
            # å¯¼å‡ºä¸º JSON
            json_data = json.dumps(answers, ensure_ascii=False, indent=2)
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½ JSON",
                data=json_data,
                file_name="filled_form.json",
                mime="application/json",
                use_container_width=True
            )


def _render_field_filling_multi(api_key: str, profiles: List[Dict], fields: List[Dict], file_type: str, form_mode: str):
    """æ¸²æŸ“å¤šäººå­—æ®µå¡«å†™ç•Œé¢ï¼ˆæ”¯æŒä¸€äººä¸€è¡¨å’Œä¸€è¡¨å¤šäººï¼‰"""
    
    st.markdown("---")
    
    if form_mode == "batch":
        st.subheader(f"ğŸ“ æ‰¹é‡å¡«å†™ ({len(profiles)} äºº)")
        _render_batch_mode(api_key, profiles, fields, file_type)
    else:
        st.subheader(f"ğŸ“ èšåˆå¡«å†™ ({len(profiles)} äºº)")
        _render_aggregate_mode(api_key, profiles, fields, file_type)


def _render_batch_mode(api_key: str, profiles: List[Dict], fields: List[Dict], file_type: str):
    """æ¸²æŸ“ä¸€äººä¸€è¡¨æ¨¡å¼ï¼ˆæ¯äººç”Ÿæˆç‹¬ç«‹ç»“æœï¼‰"""
    
    # åˆå§‹åŒ–å¤šäººç­”æ¡ˆå­˜å‚¨
    if 'multi_answers' not in st.session_state:
        st.session_state['multi_answers'] = {}
    
    multi_answers = st.session_state['multi_answers']
    
    # æ¶¦è‰²é£æ ¼é€‰æ‹©
    polish_style = st.selectbox(
        "æ¶¦è‰²é£æ ¼",
        ["professional", "academic", "friendly"],
        format_func=lambda x: {"professional": "ä¸“ä¸šæ­£å¼", "academic": "å­¦æœ¯è§„èŒƒ", "friendly": "äº²å’Œå‹å¥½"}[x],
        key="batch_polish_style"
    )

    # ========== ä¿¡æ¯å®Œæ•´æ€§æ ¡éªŒï¼šä»»ä¸€äººä¸å®Œæ•´åˆ™ç¦æ­¢ç”Ÿæˆ/å¯¼å‡º ==========
    incomplete = []
    for p in profiles:
        try:
            src = str(p.get("source", ""))
            res = validate_research_profile(p.get("data")) if "ç§‘ç ”" in src else validate_general_profile(p.get("data"))
        except Exception:
            res = {"is_complete": False, "missing_required": ["ç”»åƒç»“æ„å¼‚å¸¸"]}
        if not res.get("is_complete"):
            incomplete.append({"name": p.get("name", "æœªçŸ¥"), "missing": res.get("missing_required", [])})

    can_output = len(incomplete) == 0
    if not can_output:
        names = [x["name"] for x in incomplete]
        st.error("âš ï¸ ä»¥ä¸‹äººå‘˜å¿…å¡«ä¿¡æ¯ä¸å®Œæ•´ï¼šæ— æ³•æ‰¹é‡ç”Ÿæˆ/å¯¼å‡ºã€‚"
                 + f" äººå‘˜ï¼š{', '.join(names)}")
        with st.expander("æŸ¥çœ‹ç¼ºå¤±é¡¹"):
            for x in incomplete:
                missing = "ã€".join(x.get("missing", []) or [])
                st.markdown(f"- **{x['name']}**ï¼š{missing if missing else 'ç¼ºå¤±å¿…å¡«é¡¹'}")
    
    # æ‰¹é‡ç”Ÿæˆæ‰€æœ‰äººçš„ç­”æ¡ˆï¼ˆä¼˜åŒ–ï¼šæ¯äººä¸€æ¬¡ API è°ƒç”¨ï¼‰
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸš€ æ‰¹é‡ç”Ÿæˆæ‰€æœ‰äººçš„ç­”æ¡ˆ", use_container_width=True, key="batch_generate_all", disabled=not can_output):
            client = create_ai_client(api_key)
            progress = st.progress(0)
            
            for i, profile in enumerate(profiles):
                profile_id = profile["id"]
                profile_name = profile["name"]
                
                with st.spinner(f"æ­£åœ¨ä¸º {profile_name} æ‰¹é‡ç”Ÿæˆç­”æ¡ˆ..."):
                    # æ¯äººä¸€æ¬¡ API è°ƒç”¨
                    batch_answers = ai_batch_generate_answers(client, fields, profile["data"], polish_style)
                    
                    if batch_answers and "_error" not in batch_answers:
                        multi_answers[profile_id] = batch_answers
                    else:
                        # å›é€€åˆ°é€ä¸ªç”Ÿæˆ
                        multi_answers[profile_id] = {}
                        for field_info in fields:
                            field = field_info["field"]
                            field_type = field_info.get("type", "subjective")
                            answer = ai_generate_answer(client, field, field_type, profile["data"])
                            multi_answers[profile_id][field] = answer
                
                progress.progress((i + 1) / len(profiles))
            
            st.session_state['multi_answers'] = multi_answers
            st.success(f"âœ… å·²ä¸º {len(profiles)} äººç”Ÿæˆç­”æ¡ˆï¼")
            st.rerun()
    
    with col2:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰ç­”æ¡ˆ", use_container_width=True, key="batch_clear_all"):
            st.session_state['multi_answers'] = {}
            st.rerun()
    
    st.markdown("---")
    
    # ä¸ºæ¯ä¸ªäººæ˜¾ç¤ºç»“æœï¼ˆä½¿ç”¨ tabsï¼‰
    if profiles:
        person_tabs = st.tabs([p["name"] for p in profiles])
        
        for i, (tab, profile) in enumerate(zip(person_tabs, profiles)):
            with tab:
                profile_id = profile["id"]
                person_answers = multi_answers.get(profile_id, {})
                
                st.markdown(f"**{profile['name']}** ({profile['source']})")
                
                # æ˜¾ç¤ºæ¯ä¸ªå­—æ®µ
                for field_info in fields:
                    field = field_info["field"]
                    field_type = field_info.get("type", "subjective")
                    
                    type_badge = "ğŸ”µ" if field_type == "factual" else "ğŸŸ¢"
                    st.markdown(f"{type_badge} **{field}**")
                    
                    current_answer = person_answers.get(field, "")
                    new_answer = st.text_area(
                        f"ç­”æ¡ˆ",
                        value=current_answer,
                        key=f"batch_{profile_id}_{field}",
                        height=80,
                        label_visibility="collapsed"
                    )
                    
                    if new_answer != current_answer:
                        if profile_id not in multi_answers:
                            multi_answers[profile_id] = {}
                        multi_answers[profile_id][field] = new_answer
                        st.session_state['multi_answers'] = multi_answers
                
                # å•äººå¯¼å‡º
                if person_answers:
                    st.markdown("---")
                    if can_output:
                        csv_data = export_answers_to_csv(person_answers)
                        st.download_button(
                            label=f"ğŸ“¥ ä¸‹è½½ {profile['name']} çš„ç»“æœ (CSV)",
                            data=csv_data,
                            file_name=f"filled_form_{profile['name']}.csv",
                            mime="text/csv",
                            key=f"download_{profile_id}"
                        )
                    else:
                        st.warning("ä¿¡æ¯ä¸å®Œæ•´ï¼šå¯¼å‡ºå·²ç¦ç”¨")
    
    # æ‰¹é‡å¯¼å‡ºæ‰€æœ‰äºº
    if multi_answers:
        st.markdown("---")
        st.subheader("ğŸ“¤ æ‰¹é‡å¯¼å‡º")
        
        # åˆå¹¶æ‰€æœ‰äººçš„ç»“æœä¸ºä¸€ä¸ª JSON
        all_results = {}
        for profile in profiles:
            profile_id = profile["id"]
            if profile_id in multi_answers:
                all_results[profile["name"]] = multi_answers[profile_id]
        
        if all_results:
            if can_output:
                json_data = json.dumps(all_results, ensure_ascii=False, indent=2)
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½æ‰€æœ‰äººçš„ç»“æœ (JSON)",
                    data=json_data,
                    file_name="all_filled_forms.json",
                    mime="application/json",
                    use_container_width=True
                )
            else:
                st.warning("ä¿¡æ¯ä¸å®Œæ•´ï¼šå¯¼å‡ºå·²ç¦ç”¨")


def _render_aggregate_mode(api_key: str, profiles: List[Dict], fields: List[Dict], file_type: str):
    """æ¸²æŸ“ä¸€è¡¨å¤šäººæ¨¡å¼ï¼ˆæ‰€æœ‰äººå¡«å…¥åŒä¸€è¡¨æ ¼ï¼‰"""
    
    # åˆå§‹åŒ–èšåˆç­”æ¡ˆå­˜å‚¨
    if 'aggregate_answers' not in st.session_state:
        st.session_state['aggregate_answers'] = {}
    
    aggregate_answers = st.session_state['aggregate_answers']
    
    # æ¶¦è‰²é£æ ¼é€‰æ‹©
    polish_style = st.selectbox(
        "æ¶¦è‰²é£æ ¼",
        ["professional", "academic", "friendly"],
        format_func=lambda x: {"professional": "ä¸“ä¸šæ­£å¼", "academic": "å­¦æœ¯è§„èŒƒ", "friendly": "äº²å’Œå‹å¥½"}[x],
        key="aggregate_polish_style"
    )

    # ========== ä¿¡æ¯å®Œæ•´æ€§æ ¡éªŒï¼šä»»ä¸€äººä¸å®Œæ•´åˆ™ç¦æ­¢ç”Ÿæˆ/å¯¼å‡º ==========
    incomplete = []
    for p in profiles:
        try:
            src = str(p.get("source", ""))
            res = validate_research_profile(p.get("data")) if "ç§‘ç ”" in src else validate_general_profile(p.get("data"))
        except Exception:
            res = {"is_complete": False, "missing_required": ["ç”»åƒç»“æ„å¼‚å¸¸"]}
        if not res.get("is_complete"):
            incomplete.append({"name": p.get("name", "æœªçŸ¥"), "missing": res.get("missing_required", [])})

    can_output = len(incomplete) == 0
    if not can_output:
        names = [x["name"] for x in incomplete]
        st.error("âš ï¸ ä»¥ä¸‹äººå‘˜å¿…å¡«ä¿¡æ¯ä¸å®Œæ•´ï¼šæ— æ³•æ‰¹é‡ç”Ÿæˆ/å¯¼å‡ºã€‚"
                 + f" äººå‘˜ï¼š{', '.join(names)}")
        with st.expander("æŸ¥çœ‹ç¼ºå¤±é¡¹"):
            for x in incomplete:
                missing = "ã€".join(x.get("missing", []) or [])
                st.markdown(f"- **{x['name']}**ï¼š{missing if missing else 'ç¼ºå¤±å¿…å¡«é¡¹'}")
    
    # æ‰¹é‡ç”Ÿæˆ
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸš€ æ‰¹é‡ç”Ÿæˆæ‰€æœ‰äººçš„ç­”æ¡ˆ", use_container_width=True, key="aggregate_generate_all", disabled=not can_output):
            client = create_ai_client(api_key)
            progress = st.progress(0)
            
            for i, profile in enumerate(profiles):
                profile_id = profile["id"]
                
                with st.spinner(f"æ­£åœ¨ä¸º {profile['name']} æ‰¹é‡ç”Ÿæˆç­”æ¡ˆ..."):
                    # æ¯äººä¸€æ¬¡ API è°ƒç”¨
                    batch_answers = ai_batch_generate_answers(client, fields, profile["data"], polish_style)
                    
                    if batch_answers and "_error" not in batch_answers:
                        aggregate_answers[profile_id] = {"_name": profile["name"], **batch_answers}
                    else:
                        # å›é€€åˆ°é€ä¸ªç”Ÿæˆ
                        aggregate_answers[profile_id] = {"_name": profile["name"]}
                        for field_info in fields:
                            field = field_info["field"]
                            field_type = field_info.get("type", "subjective")
                            answer = ai_generate_answer(client, field, field_type, profile["data"])
                            aggregate_answers[profile_id][field] = answer
                
                progress.progress((i + 1) / len(profiles))
            
            st.session_state['aggregate_answers'] = aggregate_answers
            st.success(f"âœ… å·²ä¸º {len(profiles)} äººç”Ÿæˆç­”æ¡ˆï¼")
            st.rerun()
    
    with col2:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰ç­”æ¡ˆ", use_container_width=True, key="aggregate_clear_all"):
            st.session_state['aggregate_answers'] = {}
            st.rerun()
    
    st.markdown("---")
    
    # ä»¥è¡¨æ ¼å½¢å¼å±•ç¤ºæ‰€æœ‰äººçš„ç­”æ¡ˆ
    if aggregate_answers:
        st.subheader("ğŸ“Š æ±‡æ€»è¡¨æ ¼")
        
        # æ„å»º DataFrame
        table_data = []
        for profile in profiles:
            profile_id = profile["id"]
            if profile_id in aggregate_answers:
                row = {"å§“å": profile["name"]}
                for field_info in fields:
                    field = field_info["field"]
                    row[field] = aggregate_answers[profile_id].get(field, "")
                table_data.append(row)
        
        if table_data:
            df = pd.DataFrame(table_data)
            
            # ä½¿ç”¨ data_editor å…è®¸ç¼–è¾‘
            edited_df = st.data_editor(
                df,
                use_container_width=True,
                num_rows="fixed",
                key="aggregate_table_editor"
            )
            
            # æ›´æ–°ç¼–è¾‘åçš„æ•°æ®
            for i, profile in enumerate(profiles):
                if i < len(edited_df):
                    profile_id = profile["id"]
                    if profile_id not in aggregate_answers:
                        aggregate_answers[profile_id] = {"_name": profile["name"]}
                    for field_info in fields:
                        field = field_info["field"]
                        if field in edited_df.columns:
                            aggregate_answers[profile_id][field] = edited_df.iloc[i][field]
            
            st.session_state['aggregate_answers'] = aggregate_answers
            
            # å¯¼å‡º
            st.markdown("---")
            st.subheader("ğŸ“¤ å¯¼å‡ºæ±‡æ€»è¡¨")

            if not can_output:
                st.warning("ä¿¡æ¯ä¸å®Œæ•´ï¼šå¯¼å‡ºå·²ç¦ç”¨ã€‚è¯·å…ˆè¡¥å…¨å¿…å¡«ä¿¡æ¯ã€‚")
                return
            
            col_exp1, col_exp2 = st.columns(2)
            with col_exp1:
                csv_data = df.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½æ±‡æ€»è¡¨ (CSV)",
                    data=csv_data,
                    file_name="aggregated_form.csv",
                    mime="text/csv",
                    use_container_width=True
                )
            
            with col_exp2:
                # è½¬ä¸º Excel
                output = io.BytesIO()
                df.to_excel(output, index=False, engine='openpyxl')
                output.seek(0)
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½æ±‡æ€»è¡¨ (Excel)",
                    data=output.getvalue(),
                    file_name="aggregated_form.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True
                )
    else:
        # æ˜¾ç¤ºç©ºè¡¨æ ¼é¢„è§ˆ
        st.info("ç‚¹å‡»ã€Œæ‰¹é‡ç”Ÿæˆã€åï¼Œå°†åœ¨æ­¤æ˜¾ç¤ºæ‰€æœ‰äººçš„æ±‡æ€»è¡¨æ ¼")
